#  一个自动伸缩的分布式一致性hash的kv存储框架
====

草稿

## 一致性hash的本质:

在不考虑冲突的情况下(也暂时不考虑虚拟节点的情况)，   
所有的数据点位于一个离散的集合里，称之为 hash集合吧。   
将这个hash集合里面的元素按顺序进行排列，变成一个有序集合，这个有序集合就叫线性集合吧。   
这个hash集合和线性集合的元素是满射，一一对应的。   

假设有数据服务器m台。 对每一台按照与数据点一样的hash算法算出hash值。   
那么这m个数据服务器，将会把线性集合划分成 m+1个线段。当将首尾两个线段连接在一起后，会得到m个线段的集合，   
设定每一个线段都包含这样的[当前节点hash值，下一个节点hash值)  注意，是一个半开半闭空间。   
于是线段集合便和服务器集合一一映射。

于是，我们可以有以下结论：一个数据点将映射到唯一的一个线段上，并也会映射到唯一的一个数据服务器上。


##  一个自动伸缩的分布式一致性hash的kv存储框架

假如我们使用zookeeper之类的分布式锁工具，便可以针对一致性hash做一个方便的管理工具。
可以方便的对数据节点进行增加， 删除，并能够在故障的时候尽可能的减少数据损失(如果没有冷热备份，不可避免会损失)，以及重新加入时不需要担心细节。

### 客户端:
在本地维护一个数据服务状态节点的列表,
控制本地的读写接口， 根据需要，在一些特殊情况下需要两次读或者两次写。


### 数据服务器，

对应的状态

* new_uninit: 新加入,未初始化，主要是计算hash值，在zookeeper上注册节点。不影响逻辑。该状态下的节点不接受任何的读写请求。

* new_sync:数据同步状态,在改状态下，数据节点参与读写请求。 
设定一个redo的读写日志。
等待前继节点转入normal_split状态，启动从前继节点到本节点的数据同步过程。

对于该节点的所有写请求，登记到redo日志区，并写入本结点。
对于该节点所有的读请求，在本节点读取失效后，再转到前驱节点读取。这将由客户端控制。 

当读取完结后，节点状态转为 normal

* normal:正常状态

* normal_split:
当后继节点状态变为normal时，清除多余数据，并转成normal状态。

* delete_prepare:
要取消该节点，数据需要迁移到前继节点上。数据节点仍然参与读写请求。
等待前继节点变成 normal_merge状态后，自身变成delete_sync状态。


* delete_sync状态:
数据节点参与读请求，不参与写请求。
所有的该节点的读请求将转向前继节点。如果失效后，再从本节点读。
启动数据传输过程。
数据传输完成后。状态节点变成 delete_complete.

* delete_complete状态:
数据节点不参与各种请求。可以被移除。


* normal_merge状态:
设定一个redo的读写日志。
属于后续节点的写请求将写入redo日志，然后再写入数据节点。

接受数据传输，数据传输对于已有的数据将采用忽略的方式。

数据传输完成后，等待后继节点的状态变成 delete_complete,然后自身恢复成normal.


* normal_proxy状态:
当其后续节点发生了故障时，本节点检测到后，将会变成normal_proxy模式。
设定一个redo的读写日志redo1。
属于后续节点的写请求将写入redo日志，然后再写入数据节点。
直到收到管理命令，认为故障节点不可恢复后， 自身变成normal状态，废弃redo1日志。

当后继节点重新加入后，变成rejoin_recover的状态时，停止redo1的记录，同时开启redo2的记录。 
传输redo日志redo1，
传输完毕后
自身变成normal_proxy_stop状态。

采用两处写的方式，保证数据一致性。
接受读请求，当不能命中后，到后继节点进行再次读取。

(假如 故障节点无法恢复，或者还没有恢复，对于读操作可能会不正常)


* normal_proxy_stop状态：
停止接受后继节点的读写请求，清理后继节点的数据， 转换成 normal状态。

* rejoin_recover状态
故障节点重新加入时的状态，原节点hash值不变。

设定一个redo的读写日志。接受读请求，后写入本身数据节点。

接收前继节点送过来的redo1日志，与本身的redo日志做一个对比，如果不在redo日志里，则写入，如果再的丢弃。


##todo：如何与 memcached 和 redis集成。 